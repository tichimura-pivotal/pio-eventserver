#!/bin/bash

# Fail immediately on non-zero exit code.
set -e
# Fail immediately on non-zero exit code within a pipeline.
set -o pipefail
# Fail on undeclared variables.
set -u
# Debug, echo every command
#set -x

function error() {
  echo " !     $*" >&2
  exit 1
}

function topic() {
  echo "-----> $*"
}

function indent() {
  c='s/^/       /'
  case $(uname) in
    Darwin) sed -l "$c";;
    *)      sed -u "$c";;
  esac
}

# If the engine requires the newer Apache distribution
# returns 'true', and otherwise '' (empty string)
requires_apache_predictionio() {
  local template_json=$1
  if [ -e $template_json ]
  then
    cat $template_json | ruby \
      -E utf-8:utf-8 \
      -r json \
      -e "version = JSON.parse(STDIN.read)['pio']['version']['min']; major,minor = version.split('.').map(&:to_i); STDOUT << (major>=0 && minor>=10 ? 'true' : '')"
  else
    # without a template file, assume we're build the eventserver on the newest version
    echo 'true'
  fi
}

export_env_dir() {
  local env_dir=$1
  local whitelist_regex=${2:-''}
  local blacklist_regex=${3:-'^(PATH|GIT_DIR|CPATH|CPPATH|LD_PRELOAD|LIBRARY_PATH|IFS)$'}
  if [ -d "$env_dir" ]; then
    for e in $(ls $env_dir); do
      echo "$e" | grep -E "$whitelist_regex" | grep -qvE "$blacklist_regex" &&
      export "$e=$(cat $env_dir/$e)"
      :
    done
  fi
}

# parse and derive params
BUILD_DIR=${1:-}
CACHE_DIR=${2:-}
ENV_DIR=${3:-}
BP_DIR=$(cd $(dirname ${0:-}); cd ..; pwd)


if [ $(requires_apache_predictionio "$BUILD_DIR/template.json") ]
then
  PIO_VERSION=0.10.0-incubating
  SPARK_VERSION=spark-1.6.2-bin-hadoop2.6
  # No special PostgreSQL driver is required
  POSTGRESQL_DRIVER=''
  echo "Using Apache PredictionIO ${PIO_VERSION}" | indent
else
  mkdir -p "$BUILD_DIR/.heroku"
  echo '# Presence of this file indicates a pre-Apache engine (<= 0.9)' > "$BUILD_DIR/.heroku/.is_old_predictionio"
  PIO_VERSION=0.9.5
  SPARK_VERSION=spark-1.6.2-bin-hadoop2.6
  POSTGRESQL_DRIVER=postgresql-9.4.1209.jar
  echo "Using PredictionIO $PIO_VERSION" | indent
fi

PIO_BUILD=PredictionIO-${PIO_VERSION}


# PredictionIO dist tarball URL, expects `.tar.gz` 
default_url="https://marsikai.s3.amazonaws.com/${PIO_BUILD}.tar.gz"
export_env_dir "$ENV_DIR" '^PREDICTIONIO_DIST_URL$'
url="${PREDICTIONIO_DIST_URL-$default_url}"

# The PATH set in .profile.d/pio-env.sh must match.
dist_name="PredictionIO-dist"
dist_dir="$BUILD_DIR/$dist_name"

# Build with dyno runtime-compatible prefix.
# The PATH set in .profile.d/pio-env.sh must match.
# (`pio build` ends up capturing its absolute build path, so it needs
# to align with the actual runtime.)
app_namespace="pio-engine"
app_prefix="/app/${app_namespace}"
mkdir -p $app_prefix

default_pio_env="config/pio-env-12f.sh"
custom_pio_env="$BUILD_DIR/config/pio-env.sh"
target_pio_env="${dist_dir}/conf/pio-env.sh"

topic "Fetch distribution of PredictionIO"
curl -o "${dist_name}.tar.gz" -S "$url"
mkdir -p "$dist_dir"
tar -xz -f "${dist_name}.tar.gz" -C "$dist_dir" --strip-components=1 | indent

topic "Fetch distribution of Spark"
curl -o "spark-hadoop.tar.gz" -S "http://marsikai.s3.amazonaws.com/${SPARK_VERSION}.tar.gz"
mkdir -p "$dist_dir/vendors/spark-hadoop"
tar -xz -f "spark-hadoop.tar.gz" -C "$dist_dir/vendors/spark-hadoop" --strip-components=1  | indent

topic "Configure PredictionIO"
if [ -f "${custom_pio_env}" ]
then
  echo "Using custom 'config/pio-env.sh'" | indent
  cp "${custom_pio_env}" "${target_pio_env}" | indent
else
  echo "Writing default 'pio-env.sh'" | indent
  cp "${default_pio_env}" "${target_pio_env}" | indent
fi

topic "Set-up PredictionIO environment"
mkdir -p "$BUILD_DIR/.profile.d"
cp -r .profile.d/* "$BUILD_DIR/.profile.d/" | indent
# Load the env for the following build steps
export_env_dir "$ENV_DIR" '^DATABASE_URL$'
source .profile.d/pio-env.sh

if [ -f "${BUILD_DIR}/engine.json" ]
then
  topic "Build PredictionIO engine"
  # Move to dyno-compatible prefix for `pio build`
  mv $BUILD_DIR/* $app_prefix
  cd $app_prefix
  $dist_name/bin/pio build --verbose | indent

  topic "Clean-up build artifacts"
  # Try to keep slug below 300MB limit.
  # This is based on profiling with
  # `du -a "${BUILD_DIR}" | sort -n -r | head -n 50`
  # and removing big objects that seem unnecessary.
  rm -rf "target/streams" || true

  topic "Moving build artifacts into place"
  # After PIO build at the runtime prefix, move the engine into the slug
  mv $app_prefix $BUILD_DIR
  # Move the Procfile back to the top-level app directory or use default for engines
  # (`bin/release` default_process_types have no effect since this is never the last buildpack)
  if [ -f "${BUILD_DIR}/${app_namespace}/Procfile" ]
  then
    mv "${BUILD_DIR}/${app_namespace}/Procfile" $BUILD_DIR
  else
    mv "${BP_DIR}/Procfile-engine" "${BUILD_DIR}/Procfile"
  fi

  topic "Set-up train on release (disable with PIO_TRAIN_ON_RELEASE=false)"
  mkdir -p "$BUILD_DIR/bin"
  cp $BP_DIR/bin/engine/heroku-* "$BUILD_DIR/bin/"

else
  echo 'No engine to build. (`engine.json` does not exist.)' | indent
fi

if [ "$POSTGRESQL_DRIVER" ]
then
  topic "Install PostgreSQL database driver"
  mkdir -p "${BUILD_DIR}/lib"
  curl -s -L "https://marsikai.s3.amazonaws.com/$POSTGRESQL_DRIVER" > "${BUILD_DIR}/lib/postgresql_jdbc.jar"
fi